{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to compare different language models. The [dataset](https://liveproject-resources.s3.amazonaws.com/116/other/stackexchange_812k.csv.gz) is composed of over 800k questions and answers extracted from The Stack Exchange website for the CrossValidated site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 5 variables: post_id, parent_id, comment_id, text, and category. In this file, the column text is cleaned.\n",
    "\n",
    "The dataset is cleaned in the following way:\n",
    "\n",
    "- html tags are removed\n",
    "- Latex expressions are removed\n",
    "- Remove rows with value NA in column \n",
    "- Remove trailing whitespace\n",
    "- Remove digits\n",
    "- convert text to lower case\n",
    "- Tokenize sentences, by using word_tokenize from NLTK library\n",
    "- Remove words containing other than allowed characters (puncuation and letters)\n",
    "- Remove very short and very long texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('D:/liveproject_manning/stackexchange_812k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(812132, 5)\n",
      "<bound method NDFrame.head of         post_id  parent_id  comment_id  \\\n",
      "0             1        NaN         NaN   \n",
      "1             2        NaN         NaN   \n",
      "2             3        NaN         NaN   \n",
      "3             4        NaN         NaN   \n",
      "4             6        NaN         NaN   \n",
      "...         ...        ...         ...   \n",
      "812127   279994        NaN    536471.0   \n",
      "812128   279998        NaN    536439.0   \n",
      "812129   279998        NaN    536514.0   \n",
      "812130   279999        NaN    536802.0   \n",
      "812131   279999        NaN    542550.0   \n",
      "\n",
      "                                                     text category  \n",
      "0                           Eliciting priors from experts    title  \n",
      "1                                      What is normality?    title  \n",
      "2       What are some valuable Statistical Analysis op...    title  \n",
      "3       Assessing the significance of differences in d...    title  \n",
      "4       The Two Cultures: statistics vs. machine learn...    title  \n",
      "...                                                   ...      ...  \n",
      "812127  It does run, and gives very valid looking esti...  comment  \n",
      "812128  It seems to me that you are correct; the doubl...  comment  \n",
      "812129  It wouldn't be the first time a grader has mis...  comment  \n",
      "812130  The basic idea is to compare the clustering co...  comment  \n",
      "812131  As per your other question, your data does not...  comment  \n",
      "\n",
      "[812132 rows x 5 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_clean'] = data['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_html = re.compile('<.*?>')\n",
    "data['text_clean']= data['text_clean'].str.replace(re_html,'', regex = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove latex expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_latex = re.compile('\\\\$.*?\\\\$')\n",
    "data['text_clean']= data['text_clean'].str.replace(re_latex,'', regex = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rows with value NA in relevant column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['text_clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trailing whitespaces are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_clean']= data['text_clean'].str.rstrip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_digits = re.compile(r'\\d+')\n",
    "data['text_clean']= data['text_clean'].str.replace(re_digits,'', regex = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_clean']= data['text_clean'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_clean'] = data['text_clean'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of         post_id  parent_id  comment_id  \\\n",
       "0             1        NaN         NaN   \n",
       "1             2        NaN         NaN   \n",
       "2             3        NaN         NaN   \n",
       "3             4        NaN         NaN   \n",
       "4             6        NaN         NaN   \n",
       "...         ...        ...         ...   \n",
       "812127   279994        NaN    536471.0   \n",
       "812128   279998        NaN    536439.0   \n",
       "812129   279998        NaN    536514.0   \n",
       "812130   279999        NaN    536802.0   \n",
       "812131   279999        NaN    542550.0   \n",
       "\n",
       "                                                     text category  \\\n",
       "0                           Eliciting priors from experts    title   \n",
       "1                                      What is normality?    title   \n",
       "2       What are some valuable Statistical Analysis op...    title   \n",
       "3       Assessing the significance of differences in d...    title   \n",
       "4       The Two Cultures: statistics vs. machine learn...    title   \n",
       "...                                                   ...      ...   \n",
       "812127  It does run, and gives very valid looking esti...  comment   \n",
       "812128  It seems to me that you are correct; the doubl...  comment   \n",
       "812129  It wouldn't be the first time a grader has mis...  comment   \n",
       "812130  The basic idea is to compare the clustering co...  comment   \n",
       "812131  As per your other question, your data does not...  comment   \n",
       "\n",
       "                                               text_clean  \n",
       "0                      [eliciting, priors, from, experts]  \n",
       "1                                [what, is, normality, ?]  \n",
       "2       [what, are, some, valuable, statistical, analy...  \n",
       "3       [assessing, the, significance, of, differences...  \n",
       "4       [the, two, cultures, :, statistics, vs., machi...  \n",
       "...                                                   ...  \n",
       "812127  [it, does, run, ,, and, gives, very, valid, lo...  \n",
       "812128  [it, seems, to, me, that, you, are, correct, ;...  \n",
       "812129  [it, would, n't, be, the, first, time, a, grad...  \n",
       "812130  [the, basic, idea, is, to, compare, the, clust...  \n",
       "812131  [as, per, your, other, question, ,, your, data...  \n",
       "\n",
       "[812132 rows x 6 columns]>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look what kind of characters occur in the dataset. This shows that there is a lot of rubbish that needs to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'🎁', '̂', 'م', 'מ', '枚', '¨', '╩', '⊆', 'ｗ', '⏜', '≪', '임', '𝑛', 'ṽ', '≥', '𝑣', '′', '；', '𝑀', '╯', 's', 'ｒ', '\\u242a', '΄', 'ý', '▽', '😜', 'ｉ', 'ℙ', '𝐻', '六', '⎥', 'ú', 'v', '😔', 'β', '𝑋', '%', '𝑤', '½', 'ʼ', '回', 'し', 'ツ', '房', '𝛔', '║', 'щ', '₂', 'ل', 'æ', '¢', 'ʃ', '╤', '羲', '｛', '사', 'х', 'ר', '°', 'ｃ', 'α', '╝', '◊', '☃', 'ⁿ', '皮', '十', 'ώ', '�', '인', '˜', '⁻', '„', '宇', '\\u200b', 'ø', '𝑄', '捕', '𝖳', 'ơ', 'ｎ', '№', '提', '𝜒', '≠', '¥', '\\ue012', '≫', 'ῖ', '😊', '³', 'õ', '⅕', '[', '⋅', '␣', '🏼', '⁵', 'ă', '‘', '𝑚', 'ｈ', 'ℓ', '⁞', '𝑉', '►', 'ļ', '玉', '╘', 'ô', '𝑔', '😕', 'く', '个', '®', '을', '♂', 'ν', '😂', '~', 'ὶ', '𝐱', '😛', '\\uf0e0', 'ϵ', 'з', 'g', 'ρ', '̃', 'ϐ', 'ａ', '»', '₹', 'ʞ', 'ḽ', '⁶', '≤', '天', '⊙', '┤', 'て', 'φ', 'ḇ', 'ύ', '♠', '├', '⁴', '∨', 'ﬁ', 'ἀ', '麻', '∃', '╮', '¬', 'ｇ', 'ò', 'ל', '∥', '海', '⠀', '𝐴', '😅', 'ö', 'ن', 'ｖ', '\\U0010fc04', 'ц', 'ł', '⋮', '△', '湖', '˙', '•', '𝛂', '♢', 'ю', '؟', '\\U0010fc06', '𝖯', '시', '╒', 'σ', 'ɛ', 'å', 'е', 'ɐ', 'ˇ', 'も', '︵', '▀', 'û', 'ẁ', '督', '✗', '秀', '\\u202c', '₁', '+', 'و', 'n', '𝑝', '들', 'ב', 'т', '‚', 'ù', '∫', 'ʒ', '𝑈', '”', '四', '͝', 'а', '⎣', 'â', 'ɣ', '̇', '-', 'ŷ', '辰', '⁽', '🤣', 'ح', 'ʟ', '李', '⟩', '^', '‟', 'כ', '𝚽', 'ς', '¼', 'χ', 'ἡ', '🤔', '⊤', 'л', '︎', '완', 'ẃ', 'ע', '도', '🔥', '𝛿', ';', 'ï', 'н', 'ī', 'ざ', '}', '源', '/', 'ﬀ', 'ë', 'ě', '⁷', '™', 'ė', '物', '⟶', 'ч', '★', '芝', 'ì', 'ℱ', '᾽', '𝜙', 'ا', '↦', 'ĕ', 'が', 'う', '든', '∇', '∏', 'ā', '\\uf0b7', 'ή', '╚', '╭', '≔', 'ə', '）', '¯', '𝐺', 'ד', '蛋', '✓', 'й', 'ğ', '洪', 'ח', 'ズ', '\\U0010fc03', '⁄', '‖', 'ت', '𝐷', '😃', '𝜖', 'ξ', 'ﬃ', '…', '𝐾', 'ᴇ', 'ő', 'w', '{', '，', 'ἐ', '🌝', '？', '이', '𝑞', '👋', 'נ', '（', 'q', 'ά', '∠', '😬', '∶', 'あ', 'ü', 'š', 'ȳ', 'ç', '⇔', 'ಠ', 'ū', 'í', '⁎', '̸', '𝑦', '👹', 'ˌ', 'ř', '𝑍', 'ъ', 'ℕ', 'ｕ', '·', '😎', '□', '의', '装', '≃', '！', '、', 'り', '𝐿', '・', '门', '形', 'ع', 'ˈ', 'p', '😭', 'э', 'を', '造', '𝑖', '卡', '𝐲', '😳', 'ủ', 'ｍ', 'ك', '‑', '植', '━', 'ὅ', 'è', 'f', 'κ', '‽', 'μ', 'ẋ', 'ɹ', 'ĺ', 'τ', '🙂', 'ɑ', '\\uf071', 'ą', '만', '𝑒', '̊', '₄', '‡', '⋯', 'ş', '오', '実', '统', 'ي', '니', '∂', 'ᴏ', '−', '>', '@', 'ㅅ', 'ṓ', 'み', '👌', '|', 'ₙ', '\\uf72b', 'た', 'ć', 'ｄ', 'á', '𝑃', '方', 'b', '¶', '\\uf0e8', 'ε', 'δ', 'ƒ', '😉', '⅔', '𝜀', 'ご', 'ế', 'œ', '⎦', '∴', 'ů', '😝', '♯', '■', '́', '→', 'e', '죄', '₀', 'λ', '\\U0010fc0a', 'о', '―', '♡', '⊂', 'у', '?', '십', 'm', '←', 'ʘ', '𝜔', '步', '̄', '∑', '𝑢', 'υ', 't', ')', '𝜇', '盒', '=', '┻', 'є', '花', 'ɥ', '∣', '÷', 'ה', '보', '▲', '𝐰', 'ק', '厨', '&', '九', 'ª', 'س', 'ı', '褒', '│', '代', 'η', '𝑊', '\\u242f', '☺', '五', '新', '`', 'ひ', '。', '⋱', '𝑑', '𝑇', '𝑌', '∀', '═', '\\u2061', 'よ', 'י', 'ｂ', '±', '💯', '¦', '｝', 'ῦ', 'ᵢ', '𝜋', '💎', '냅', 'ń', '帰', 'π', 'à', '⅓', '🤪', '¡', 'ɔ', '╠', '합', 'ז', '\\U00100000', ':', 'ŧ', '𝓁', 'd', '𝑧', '疆', '𝑏', 'א', '️', '₃', '：', '❤', 'п', '\\uf03d', 'イ', 'ã', '【', '♥', '̧', 'ᴘ', 'ʎ', '£', '▪', '𝑓', '쾌', '현', '\\u2439', '̅', '\\U0010fc20', 'ɪ', '∉', '↵', '†', 'ₐ', 'y', '↓', 'h', '松', '⁰', '̶', '╞', '\\U0010fc14', '線', '𝑐', 'ê', '모', '⇝', '\\u200e', 'м', '하', '⊥', '𝑎', 'ｋ', '∕', 'ó', '🎓', 'ベ', '─', '第', '\\u2060', '🎉', 'ϕ', 'ے', 'ｐ', '∅', '∪', 'ℒ', 'ᴅ', '𝜆', 'έ', '╔', '𝑆', 'j', '⁸', '❌', 'ὑ', 'ˉ', '𝑅', 'ₓ', 'ˋ', 'ž', '╪', '⃗', '²', 'ę', '事', '송', '̀', '\\xad', 'ω', 'ῡ', '南', '₋', '𝐸', '👍', '신', 'ð', '_', 'º', '𝜈', '\\ufeff', 'ὁ', '三', '𝔼', 'ة', '𝑁', '《', '🎈', '♦', '€', 'ʊ', '’', '<', '∝', '람', '𝒩', 'ψ', 'ₑ', '宓', '⇒', '″', 'ἕ', '🤐', '∩', '\\uf0ed', '将', '》', '谢', '⁹', '😁', 'ₔ', '营', 'ｅ', '(', '≈', '糊', '\\U0010fc10', '×', '𝛽', '〗', '😱', '§', 'ｑ', 'ι', '心', 'ο', '다', '𝜂', '\\uf0de', '\\x7f', 'k', '∗', 'я', 'ō', '𝅘𝅥𝅮', '\\U0010fc00', '✔', '𝑥', '▄', '𝛼', 'ｓ', '랑', '中', 'î', 'め', '⟺', '´', '#', '𝑠', '\\\\', 'с', 'ب', 'ό', '𝑗', '⎤', 'ь', ';', '«', \"'\", '‰', '∞', 'ț', '김', '∼', '∘', '𝐼', 'ы', 'в', 'ر', 'é', '╛', '℃', 'ὲ', 'ｆ', 'ƴ', 'р', '≰', '\\u200c', 'ｌ', '$', '‐', '\\uf700', '𝐘', '件', '𝑙', '🙁', '▒', '📚', '٪', '￼', '⁺', '“', 'ˆ', '*', ']', '—', 'ш', 'ф', '\\u200d', 'c', 'ן', '–', 'پ', 'г', '≏', 'u', '🙇', 'i', '속', 'ى', '♣', 'д', 'ℤ', '⎡', '\\u200f', 'γ', 'ä', '𝜃', 'б', '╦', '处', '＋', 'ど', 'и', 'ｔ', 'ﬂ', '∆', '히', '𝑜', '\\ue001', '̈', 'い', '\\uf07d', 'ί', '¹', 'l', '\\ue013', 'ת', '🤗', '╕', 'ś', 'ｏ', 'č', '𝜎', 'ē', '₅', '⎢', '╣', '𝐹', '╗', 'と', '˚', '∈', '－', 'θ', '╧', 'o', '上', 'ё', '𝑟', '领', '¿', 'ℎ', 'ź', '⚽', 'z', 'r', 'к', '🤷', '巡', 'ו', '𝐶', '𝜷', '😀', '归', '\\U0010fc01', '┼', 'れ', 'ʇ', '哲', '©', '内', '╬', '𝚯', 'ه', 'a', 'ñ', 'µ', 'ま', 'x', '↑', '改', ',', '⍺', '√', '℗', '≡', '你', '𝔔', '𝑡', '】', '😩', 'ζ', 'ж', 'ḙ', '🏻', '∙', '‾', '军', '!', '╡', '¾', '𝑘', '𝐵', '.', '\\uf04a', 'ß', '＠', '理', 'ɵ', '士', 'ℝ', '⟨', '✘', '⩾', 'ɸ', '∓', '〖'}\n"
     ]
    }
   ],
   "source": [
    "char_set = set()\n",
    "\n",
    "all_strings = list(data['text_clean'])\n",
    "\n",
    "for text in all_strings:\n",
    "    for word in text:\n",
    "        for char in word:\n",
    "            char_set.add(char)\n",
    "        \n",
    "print(char_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "870"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(char_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we clean the dataset radically, by selecting only words in each text that consist completely of a number of allowed characters (these are letters and '.', ',', '!', ':', ';', '?'). We can always decide later to make a different choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define allowed characters, using the ord() value.\n",
    "punctuation_and_letters = [33, 44, 46, 58, 59, 63, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, \n",
    "                          110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a list of lists is made, consisting of all tokenized and cleaned characters. Of course, it would be more elegant if this was done directly in the dataframe, but I could not make this working efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_texts = []\n",
    "\n",
    "all_texts = list(data['text_clean'])\n",
    "\n",
    "for text in all_texts:\n",
    "    \n",
    "    new_text = []\n",
    "    for word in text:\n",
    "        only_allowed = True\n",
    "        for char in word:    \n",
    "            if ord(char) not in punctuation_and_letters:\n",
    "                only_allowed = False\n",
    "                \n",
    "        if only_allowed:\n",
    "            new_text.append(word)\n",
    "            \n",
    "    new_texts.append(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now reconstruct complete sentences from lists with tokenized sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_texts = [' '.join(tokenized_list) for tokenized_list in new_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.assign(cleaned_texts=cleaned_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the intermediate cilumn 'texts_clean'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['text_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of         post_id  parent_id  comment_id  \\\n",
       "0             1        NaN         NaN   \n",
       "1             2        NaN         NaN   \n",
       "2             3        NaN         NaN   \n",
       "3             4        NaN         NaN   \n",
       "4             6        NaN         NaN   \n",
       "...         ...        ...         ...   \n",
       "812127   279994        NaN    536471.0   \n",
       "812128   279998        NaN    536439.0   \n",
       "812129   279998        NaN    536514.0   \n",
       "812130   279999        NaN    536802.0   \n",
       "812131   279999        NaN    542550.0   \n",
       "\n",
       "                                                     text category  \\\n",
       "0                           Eliciting priors from experts    title   \n",
       "1                                      What is normality?    title   \n",
       "2       What are some valuable Statistical Analysis op...    title   \n",
       "3       Assessing the significance of differences in d...    title   \n",
       "4       The Two Cultures: statistics vs. machine learn...    title   \n",
       "...                                                   ...      ...   \n",
       "812127  It does run, and gives very valid looking esti...  comment   \n",
       "812128  It seems to me that you are correct; the doubl...  comment   \n",
       "812129  It wouldn't be the first time a grader has mis...  comment   \n",
       "812130  The basic idea is to compare the clustering co...  comment   \n",
       "812131  As per your other question, your data does not...  comment   \n",
       "\n",
       "                                            cleaned_texts  \n",
       "0                           eliciting priors from experts  \n",
       "1                                     what is normality ?  \n",
       "2       what are some valuable statistical analysis op...  \n",
       "3       assessing the significance of differences in d...  \n",
       "4       the two cultures : statistics vs. machine lear...  \n",
       "...                                                   ...  \n",
       "812127  it does run , and gives very valid looking est...  \n",
       "812128  it seems to me that you are correct ; the doub...  \n",
       "812129  it would be the first time a grader has missed...  \n",
       "812130  the basic idea is to compare the clustering , ...  \n",
       "812131  as per your other question , your data does no...  \n",
       "\n",
       "[812132 rows x 6 columns]>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, remove very short and very long texts. This meand that texts fith fewer than 25 characters or more than 2000 characters are removed. If needed, this can be adapted in the process of training models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (data['cleaned_texts'].str.len() >= 25) & (data['cleaned_texts'].str.len() <= 2000)\n",
    "data = data.loc[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(779478, 6)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of         post_id  parent_id  comment_id  \\\n",
       "0             1        NaN         NaN   \n",
       "2             3        NaN         NaN   \n",
       "3             4        NaN         NaN   \n",
       "4             6        NaN         NaN   \n",
       "5             7        NaN         NaN   \n",
       "...         ...        ...         ...   \n",
       "812127   279994        NaN    536471.0   \n",
       "812128   279998        NaN    536439.0   \n",
       "812129   279998        NaN    536514.0   \n",
       "812130   279999        NaN    536802.0   \n",
       "812131   279999        NaN    542550.0   \n",
       "\n",
       "                                                     text category  \\\n",
       "0                           Eliciting priors from experts    title   \n",
       "2       What are some valuable Statistical Analysis op...    title   \n",
       "3       Assessing the significance of differences in d...    title   \n",
       "4       The Two Cultures: statistics vs. machine learn...    title   \n",
       "5                  Locating freely available data samples    title   \n",
       "...                                                   ...      ...   \n",
       "812127  It does run, and gives very valid looking esti...  comment   \n",
       "812128  It seems to me that you are correct; the doubl...  comment   \n",
       "812129  It wouldn't be the first time a grader has mis...  comment   \n",
       "812130  The basic idea is to compare the clustering co...  comment   \n",
       "812131  As per your other question, your data does not...  comment   \n",
       "\n",
       "                                            cleaned_texts  \n",
       "0                           eliciting priors from experts  \n",
       "2       what are some valuable statistical analysis op...  \n",
       "3       assessing the significance of differences in d...  \n",
       "4       the two cultures : statistics vs. machine lear...  \n",
       "5                  locating freely available data samples  \n",
       "...                                                   ...  \n",
       "812127  it does run , and gives very valid looking est...  \n",
       "812128  it seems to me that you are correct ; the doub...  \n",
       "812129  it would be the first time a grader has missed...  \n",
       "812130  the basic idea is to compare the clustering , ...  \n",
       "812131  as per your other question , your data does no...  \n",
       "\n",
       "[779478 rows x 6 columns]>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eliciting priors from experts',\n",
       " 'what are some valuable statistical analysis open source projects ?',\n",
       " 'assessing the significance of differences in distributions',\n",
       " 'the two cultures : statistics vs. machine learning ?',\n",
       " 'locating freely available data samples',\n",
       " 'so how many staticians it take to screw in a lightbulb ?',\n",
       " 'under what conditions should likert scales be used as ordinal or interval data ?',\n",
       " 'multivariate interpolation approaches',\n",
       " 'forecasting demographic census',\n",
       " 'bayesian and frequentist reasoning in plain english',\n",
       " 'finding the pdf given the cdf',\n",
       " 'tools for modeling financial time series',\n",
       " 'what is a standard deviation ?',\n",
       " 'testing random variate generation algorithms',\n",
       " 'what is the meaning of p values and t values in statistical tests ?',\n",
       " 'r packages for seasonality analysis',\n",
       " 'examples for teaching : correlation does not mean causation',\n",
       " 'number generation algorithms',\n",
       " 'explain data visualization',\n",
       " 'clustering of large , dataset',\n",
       " 'pca on correlation or covariance ?',\n",
       " 'why do us and uk schools teach different methods of calculating the standard deviation ?',\n",
       " 'can someone please explain the algorithm ?',\n",
       " 'what r packages do you find most useful in your daily work ?',\n",
       " 'where can i find useful r tutorials with various implementations ?',\n",
       " 'robust nonparametric estimation of functions based on low count data',\n",
       " 'how large a difference can be expected between standard garch and asymmetric garch volatility forecasts ?',\n",
       " 'what are good basic statistics to use for ordinal data ?',\n",
       " 'what is your favorite data visualization blog ?',\n",
       " 'power of holm multiple comparison testing compared to others',\n",
       " 'what are some good frameworks for method selection ?',\n",
       " 'what statistical blogs would you recommend ?',\n",
       " 'why square the difference instead of taking the absolute value in standard deviation ?',\n",
       " 'what is the best introductory bayesian statistics textbook ?',\n",
       " 'clojure versus r : advantages and disadvantages for data analysis',\n",
       " 'algorithms to compute the running median ?',\n",
       " 'free resources for learning r',\n",
       " 'can one use multiple regression to predict one principal component pc from several other pcs ?',\n",
       " 'is there a standard method to deal with label switching problem in mcmc estimation of mixture models ?',\n",
       " 'how would you explain markov chain monte carlo mcmc to a layperson ?',\n",
       " 'free statistical textbooks',\n",
       " 'time series for count data , with counts',\n",
       " 'how should outliers be dealt with in linear regression analysis ?',\n",
       " 'how to choose the number of hidden layers and nodes in a feedforward neural network ?',\n",
       " 'what tools could be used for applying clustering algorithms on movielens ?',\n",
       " 'cross tabulation of two categorical variables : recommended techniques',\n",
       " 'data mining how to tell whether the pattern extracted is meaningful ?',\n",
       " 'open source tools for visualizing data ?',\n",
       " 'group differences on a five point likert item',\n",
       " 'what is the difference between discrete data and continuous data ?',\n",
       " 'what is the best way to identify outliers in multivariate data ?',\n",
       " 'what are principal component scores ?',\n",
       " 'intro to statistics for an md ?',\n",
       " 'recommended visualization libraries for standalone applications',\n",
       " 'using time series analysis to violent behavior',\n",
       " 'what is the easiest way to create plots under linux ?',\n",
       " 'how do i calculate if the degree of overlap between two lists is significant ?',\n",
       " 'what is the difference between a population and a sample ?',\n",
       " 'how to deal with the effect of the order of observations in a non hierarchical cluster analysis ?',\n",
       " 'what is the between method of moments and gmm ?',\n",
       " 'resources for learning stata',\n",
       " 'in linear regression , when is it appropriate to use the log of an independent variable instead of the actual values ?',\n",
       " 'how does gentle boosting differ from adaboost ?',\n",
       " 'resources for learning about the statistical analysis of financial data',\n",
       " 'what is a good algorithm for estimating the median of a huge data set ?',\n",
       " 'the trinity of tests in maximum likelihood : what to do when faced with contradicting conclusions ?',\n",
       " 'what is the single most influential book every statistician should read ?',\n",
       " 'what are the key statistical concepts that relate to data mining ?',\n",
       " 'the monty hall problem where does our intuition fail us ?',\n",
       " 'testing and proving the randomness of numbers',\n",
       " 'how to tell if something happened in a data set which monitors a value over time',\n",
       " 'motivation for kolmogorov distance between distributions',\n",
       " 'why is ransac most widely used in statistics ?',\n",
       " 'what book would you recommend for scientists ?',\n",
       " 'what is your favorite data analysis cartoon ?',\n",
       " 'always report robust white standard errors ?',\n",
       " 'mathematical statistics videos',\n",
       " 'negative values for aicc corrected akaike information criterion',\n",
       " 'variable selection procedure for binary classification',\n",
       " 'dubious use of signal processing principles to identify a trend',\n",
       " 'in sas , how do you copy paste from the output window ?',\n",
       " 'what is the best method for checking convergence in mcmc ?',\n",
       " 'unsupervised , supervised and learning',\n",
       " 'what ways are there to show two analytical methods are equivalent ?',\n",
       " 'under what conditions does correlation imply causation ?',\n",
       " 'does it ever make sense to treat categorical data as continuous ?',\n",
       " 'why is anova taught used as if it is a different research methodology compared to linear regression ?',\n",
       " 'when to use multiple models for prediction ?',\n",
       " 'what is an instrumental variable ?',\n",
       " 'what graphical techniques are used in structural equation modeling ?',\n",
       " 'how to understand a convolutional deep belief network for audio classification ?',\n",
       " 'for within subjects tests ?',\n",
       " 'is there any reason to prefer the aic or bic over the other ?',\n",
       " 'what are the differences between the algorithm and viterbi training ?',\n",
       " 'to what extent can we call a geometric distribution a geometric density',\n",
       " ', is there an intuitive explanation ?',\n",
       " 'something like for discriminative models ?',\n",
       " 'heuristics for optimizing ?',\n",
       " 'why prediction of a predicted variable from a discriminant analysis is imperfect',\n",
       " 'how can one empirically demonstrate in r which methods the aic and bic are equivalent to ?']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['cleaned_texts'].to_list()[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the cleaned dataset as csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('cleaned_dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
